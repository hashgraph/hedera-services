version: 2.1

commands:
  install-tools:
    description: Install tools for JRS regression
    steps:
      - run:
          name: Install necessary tools
          command: |
            sudo apt update -y;
            sudo apt install -y net-tools;
            sudo apt-get install -y apt-utils;
            sudo apt install -y curl;
            sudo apt install -y python3.7;
            sudo rm /usr/bin/python3;
            sudo ln -s python3.7 /usr/bin/python3;
            sudo apt install -y python3-pip;
            pip3 install --upgrade pip;
            pip3 -q install matplotlib;
            sudo apt-get install -y python3-setuptools;
            pip3 install awscli;
      - run:
          name: Install gcloud cli
          command: |
            apt-get update || apt-get update;
            curl https://sdk.cloud.google.com > install.sh;
            bash install.sh --disable-prompts;
      - run:
          name: Authenticate gcloud cli
          command: |
            echo 'export PATH=/root/google-cloud-sdk/bin:$PATH' >> /root/.bashrc;
            source /root/.bashrc; source /root/google-cloud-sdk/completion.bash.inc;
            source /root/google-cloud-sdk/path.bash.inc;
            echo $GCLOUD_SERVICE_KEY > /tmp/gcloud-service-key.json;
            gcloud auth activate-service-account --key-file=/tmp/gcloud-service-key.json;
            gcloud --quiet config set project ${GOOGLE_PROJECT_ID};
            gcloud --quiet config set compute/zone ${GOOGLE_COMPUTE_ZONE};
            sudo apt-get install gcc python3-dev python3-setuptools -y;
            sudo pip3 uninstall crcmod;
            sudo pip3 install --no-cache-dir -U crcmod;



  ######################################################################################################################
  # Command Name:                     gcp_storage_rsync
  # Command Version:                  1.0
  #
  # Parameters:
  #       src                         the source local or remote path, mandatory
  #       dest                        the destination local or remote path, mandatory
  #
  #
  # Description:
  #
  #   Copies files or folders (recursively) from the path specified by `src` parameter to the path specified by the
  #   `dest` parameter. The paths specified by both the `src` and `dest` parameters may be local or remote paths.
  #
  #   This command uses a differential rsync and will only transfer new or changed files.
  #
  #   Example Supported Paths:
  #       - /home/myPath
  #       - /home/myPath/file.txt
  #       - gs://my-bucket/folder
  #       - gs://my-bucket/file.txt
  #
  #   NOTE: The build will fail if the `gcloud` command is not available or no CRC32 system library is available.
  #
  # Command Requirements:
  #   - gcloud (>= 323.0.0)
  #
  ######################################################################################################################
  gcp_storage_rsync:
    parameters:
      src:
        type: string
      dest:
        type: string
    steps:
      - run:
          name: "GCP Storage RSync [Source: '<< parameters.src >>', Destination: '<< parameters.dest >>']"
          command: |
            set -x;
            cd /swirlds-platform/regression
            source /root/.bashrc; source /root/google-cloud-sdk/completion.bash.inc; source /root/google-cloud-sdk/path.bash.inc;
            gsutil -m rsync -r -d "<< parameters.src >>" "<< parameters.dest >>"

  ######################################################################################################################
  # Command Name:                     gcp_import_credentials
  # Command Version:                  1.0
  #
  # Parameters:
  #       username_variable           the name of the bash environment variable holding the user name, optional
  #       keyfile_variable            the name of the bash environment variable holding the key file, optional
  #       project_variable            the name of the bash environment variable holding the project name, optional
  #
  #
  # Description:
  #
  #   Imports the gcloud keypair defined in the CircleCI environment variables specified by the
  #   `username_variable`, `keyfile_variable`, and `project_variable` parameters.
  #   The default variable names `GCP_USER_NAME`, `GCP_KEY_FILE`, and `GCP_PROJECT_ID` will be used if not specified.
  #
  #   NOTE: The build will fail if the `gcloud` command is not available or the environment variable is not
  #         properly formatted.
  #
  # Command Requirements:
  #   - gcloud (>= 323.0.0)
  #
  ######################################################################################################################
  gcp_import_credentials:
    parameters:
      username_variable:
        type: string
        default: "GCP_USER_NAME"
      keyfile_variable:
        type: string
        default: "GCP_KEY_FILE"
      project_variable:
        type: string
        default: "GCP_PROJECT_ID"
    steps:
      - run:
          name: Import the Google Cloud Service  Account
          command: |
            set -x
            source /root/.bashrc; source /root/google-cloud-sdk/completion.bash.inc; source /root/google-cloud-sdk/path.bash.inc
            echo "${<< parameters.keyfile_variable >>}" > "/tmp/gcloud_account_key.json"
            set -x
            gcloud auth activate-service-account "${<< parameters.username_variable >>}" --key-file="/tmp/gcloud_account_key.json" --project="${<< parameters.project_variable >>}"


  ######################################################################################################################
  # Command Name:                     jrs_history_retrieve
  # Command Version:                  1.0
  #
  # Parameters:
  #       bucket_name                 the name of the GCP bucket containing the JRS summary history files, mandatory
  #       summary_path                the local path of the JRS summary history files, mandatory
  #
  #
  # Description:
  #
  #   Retrieves the JRS Summary History files from prior runs using a combination of CircleCI caching and GCP bucket
  #   storage as specified by the `bucket_name` parameter. The local summary history path specified by the
  #   `summary_path` parameter will be created if it does not exist.
  #
  #   The GCP bucket should be a dedicated storage bucket that must only contain JRS summary history files.
  #
  #   NOTE: The build will fail if the `gcloud` command is not available or no CRC32 system library is available.
  #
  # Command Requirements:
  #   - gcloud (>= 323.0.0)
  #
  ######################################################################################################################
  jrs_history_retrieve:
    parameters:
      bucket_name:
        type: string
      summary_path:
        type: string
    steps:
      - run:
          name: Ensure JRS Summary Folder Exists
          command: |
            set -x
            cd /swirlds-platform/regression
            if [[ ! -d "<< parameters.summary_path >>" ]]; then
              mkdir -p "<< parameters.summary_path >>"
            fi
      - restore_cache:
          name: Restoring JRS Summary History Cache
          keys:
            - v1-jrs-summary-history-{{ epoch }}
            - v1-jrs-summary-history-
      - gcp_storage_rsync:
          src: "gs://<< parameters.bucket_name >>/"
          dest: "<< parameters.summary_path >>"


  ######################################################################################################################
  # Command Name:                     jrs_history_store
  # Command Version:                  1.0
  #
  # Parameters:
  #       bucket_name                 the name of the GCP bucket containing the JRS summary history files, mandatory
  #       summary_path                the local path of the JRS summary history files, mandatory
  #
  #
  # Description:
  #
  #   Saves the JRS Summary History files from the current run using a combination of CircleCI caching and GCP bucket
  #   storage as specified by the `bucket_name` parameter. The local summary history path specified by the
  #   `summary_path` parameter must exist.
  #
  #   The GCP bucket should be a dedicated storage bucket that must only contain JRS summary history files.
  #
  #   NOTE: The build will fail if the `gcloud` command is not available or no CRC32 system library is available.
  #
  # Command Requirements:
  #   - gcloud (>= 323.0.0)
  #
  ######################################################################################################################
  jrs_history_store:
    parameters:
      bucket_name:
        type: string
      summary_path:
        type: string
    steps:
      - save_cache:
          name: Saving JRS Summary History Cache
          key: v1-jrs-summary-history-{{ epoch }}
          paths:
            - << parameters.summary_path >>
          when: always
      - gcp_storage_rsync:
          src: "<< parameters.summary_path >>"
          dest: "gs://<< parameters.bucket_name >>/"


  ######################################################################################################################
  # Command Name:                     jrs_results_store
  # Command Version:                  1.0
  #
  # Parameters:
  #       bucket_name                 the name of the GCP bucket containing the JRS regression results, mandatory
  #       result_path                 the local path of the JRS regression result files, mandatory
  #
  #
  # Description:
  #
  #   Saves the JRS Regression result files from the current run to the GCP bucket storage as specified by the
  #   `bucket_name` parameter. The local result path specified by the `result_path` parameter must exist.
  #
  #   The GCP bucket should be a dedicated storage bucket that must only contain JRS Regression result files.
  #
  #   The final path used to the store the result files in the GCP bucket is as follows:
  #       - gs://${BUCKET_NAME}/${JRS_USER}/${JRS_BRANCH}
  #
  #   The ${JRS_USER} variable will default to a value of `hedera-services-automation` if not otherwise provided by CircleCI.
  #   This will be the case with all nightly or other jobs run via the CircleCI cron scheduler. All nightly automated
  #   regression runs should have results stored under this default username.
  #
  #   The ${JRS_BRANCH} variable will be set to the actual branch name provided by the ${CIRCLE_BRANCH} variable, if
  #   available, otherwise it will default to the tag name specified by the ${CIRCLE_TAG} variable, if available. If
  #   neither the ${CIRCLE_BRANCH} or ${CIRCLE_TAG} values are available then then CircleCI job name provided by the
  #   ${CIRCLE_JOB} variable will be used.
  #
  #   NOTE: The build will fail if the `gcloud` command is not available or no CRC32 system library is available.
  #
  # Command Requirements:
  #   - gcloud (>= 323.0.0)
  #
  ######################################################################################################################
  jrs_results_store:
    parameters:
      bucket_name:
        type: string
      result_path:
        type: string
    steps:
      - run:
          name: Saving JRS Regression Results
          command: |
            set -x
            cd /swirlds-platform/regression;
            source /root/.bashrc; source /root/google-cloud-sdk/completion.bash.inc; source /root/google-cloud-sdk/path.bash.inc;

            if [[ ! -d "<< parameters.result_path >>" ]]; then exit 13; fi

            if [[ -z "${CIRCLE_USERNAME}" ]]; then
              JRS_USER="hedera-services-automation"
            else
              JRS_USER="${CIRCLE_USERNAME}"
            fi

            if [[ -z "${CIRCLE_BRANCH}" ]]; then
              if [[ -n "${CIRCLE_TAG}" ]]; then
                JRS_BRANCH="${CIRCLE_TAG}"
              else
                JRS_BRANCH="${CIRCLE_JOB}"
              fi
            else
              JRS_BRANCH="${CIRCLE_BRANCH}"
            fi

            gsutil -m rsync -r "<< parameters.result_path >>" "gs://<< parameters.bucket_name >>/${JRS_USER}/${JRS_BRANCH}/"

            tar -czvf /results.tar.gz  /swirlds-platform/regression/<< parameters.result_path >>
          when: always



  ######################################################################################################################
  # Command Name:                     jrs_regression_execute
  # Command Version:                  1.0
  #
  # Parameters:
  #       config_file                 the relative path to the JRS regression config, mandatory
  #       regression_path             the local path to the regression folder, optional (default: "regression")
  #       slack_results_channel       the slack test results channel override, optional (default: "")
  #       slack_summary_channel       the slack summary channel override, optional (default: "")
  #
  #
  # Description:
  #
  #   Executes the JRS regression run using the provided `config_file` parameter. The `config_file` parameter is treated
  #   as a path relative to the `regression_path` parameter. The `regression_path` parameter may be either a path
  #   relative to the current working directory or an absolute path.
  #
  #   The optional `slack_results_channel` and `slack_summary_channel` parameters allow using an alternate slack channel
  #   for the respective notification types. If one or both of these are provided, then the related setting in the JSON
  #   files will be ignored.
  #
  #   This command depends on the following CircleCI Context environment variables:
  #     - JRS_SSH_USER_NAME:  the username associated with the `JRS_SSH_KEY_FILE` used by JRS to connect to the remote instances
  #     - JRS_SSH_KEY_FILE:   the base64 encoded private SSH key used by JRS to connect to remote instances
  #     - JRS_WEB_HOSTNAME:   the IP address or hostname of the JRS web server
  #     - JRS_WEB_PORT:       the port number on which the JRS web server is listening
  #
  #
  #   NOTE: The build will fail if the `gcloud` command is not available.
  #
  # Command Requirements:
  #   - gcloud (>= 323.0.0)
  #   - openjdk (>= 12.0.2)
  #
  ######################################################################################################################
  jrs_regression_execute:
    parameters:
      config_file:
        type: string
      regression_path:
        type: string
        default: "regression"
      slack_results_channel:
        type: string
        default: ""
      slack_summary_channel:
        type: string
        default: ""
      hedera_services_path:
        type: string
        default: "/repo"
    steps:
      - run:
          name: Configure JRS Regression Keys
          command: |
            set -x
            cp ~/.ssh/id_rsa_e7a63e343ed8fe642c7fb657450344ac /tmp/jrs-ssh-keyfile
            cp "/tmp/jrs-ssh-keyfile" "/tmp/jrs-ssh-keyfile.pem"
            chmod 0600 /tmp/jrs-ssh-keyfile*
            ssh-keygen -p -N "" -m pem -f "/tmp/jrs-ssh-keyfile.pem"
            ssh-keygen -y -f "/tmp/jrs-ssh-keyfile" > "/tmp/jrs-ssh-keyfile.pub"

      - run:
          name: "Execute JRS Regression (<< parameters.config_file >>)"
          command: |
            set -x
            source /root/.bashrc; source /root/google-cloud-sdk/completion.bash.inc; source /root/google-cloud-sdk/path.bash.inc;
            cd /swirlds-platform
            REGRESSION_PATH="<< parameters.regression_path >>"
            if [[ ! -d "${REGRESSION_PATH}" ]]; then exit 15; fi

            if [[ -z "${CIRCLE_USERNAME}" ]]; then
              JRS_USER="hedera-services-automation"
            else
              JRS_USER="${CIRCLE_USERNAME}"
            fi

            if [[ -z "${CIRCLE_BRANCH}" ]]; then
              if [[ -n "${CIRCLE_TAG}" ]]; then
                JRS_BRANCH="${CIRCLE_TAG}"
              else
                JRS_BRANCH="${CIRCLE_JOB}"
              fi
            else
              JRS_BRANCH="${CIRCLE_BRANCH}"
            fi

            JRS_OPTIONS=""
            SLACK_SUMMARY="<< parameters.slack_summary_channel >>"
            SLACK_RESULTS="<< parameters.slack_results_channel >>"

            if [[ -n "${SLACK_SUMMARY}" ]]; then
              JRS_OPTIONS="${JRS_OPTIONS} -Djrs.circleci.slack.summary=${SLACK_SUMMARY}"
            fi

            if [[ -n "${SLACK_RESULTS}" ]]; then
              JRS_OPTIONS="${JRS_OPTIONS} -Djrs.circleci.slack.results=${SLACK_RESULTS}"
            fi


            pushd "${REGRESSION_PATH}" > /dev/null 2>&1
              CONFIG_PATH="<< parameters.config_file >>"
              if [[ ! -f "${CONFIG_PATH}" ]]; then
                echo
                echo "Configuration File '${CONFIG_PATH}' does not exist......"
                echo
                exit 20
              fi

              if [[ -z "${JAVA_OPTS}" ]]; then
                JAVA_OPTS="-Xmx4g"
              fi

              java ${JAVA_OPTS} \
              -Djrs.circleci=true \
              -Djrs.circleci.user="${JRS_USER}" \
              -Djrs.circleci.branch="${JRS_BRANCH}" \
              -Djrs.circleci.ssh.login="${JRS_SSH_USER_NAME}" \
              -Djrs.circleci.ssh.keyfile="/tmp/jrs-ssh-keyfile" \
              ${JRS_OPTIONS} \
              -Dlog4j.configurationFile=log4j2-jrs.xml \
              -Dspring.output.ansi.enabled=ALWAYS \
              -jar regression.jar "${CONFIG_PATH}" "<< parameters.hedera_services_path >>"
            popd > /dev/null 2>&1
          no_output_timeout: 30m

  ensure-env-vars:
    description: Ensure the builtin CircleCI env vars are available
    steps:
      - run:
          name: Ensure the builtin CircleCI env vars are available
          command: |
            /repo/.circleci/scripts/ensure-builtin-env-vars.sh

  send-report-to-slack:
    parameters:
      workflow-name:
        description: name of the workflow
        type: string
        default: "continuous integration"
      slack-channel:
        description: name of the slack channel to send report to
        type: string
        default: "hedera-cicd"
      status:
        description: status of the workflow
        type: string
        default: "Passed"
    steps:
      - run:
          name: Send a successful report to slack
          command: |
            /repo/.circleci/scripts/prepare-slack-message.sh \
                "<< parameters.workflow-name >>"
            echo "<< parameters.status >>"  >> /repo/diagnostics/slack_msg.txt
            /repo/.circleci/scripts/call-svcs-app-slack.sh \
                -c << parameters.slack-channel >> \
                -t /repo/diagnostics/slack_msg.txt \
                -s << parameters.status >>


  validate-feature-update-test-report:
    description: Validate feature update test result
    parameters:
      slack-channel:
        description: slack channel to publish to
        type: string
        default: 'hedera-cicd'
      status:
        description: test finishing status
        type: string
        default: 'Passed'
      workflow-name:
        description: test finishing status
        type: string
        default: 'none-workflow'

    steps:
      - run:
          name: Final summary of workflow
          command: |
            /repo/.circleci/scripts/final-summary.sh \
                << parameters.workflow-name >>

      - run:
          name: 'send feature update test result to slack'
          command: |
            /repo/.circleci/scripts/call-svcs-app-slack.sh \
                -c << parameters.slack-channel >> \
                -t /repo/client-logs/feature-update-regression-report.txt \
                -s << parameters.status >>

  validate-record-streams:
    description: Download and validate the record streaming data
    parameters:
      perf-run:
        description: If in perf run, the output handling needs to be tolerant of small portion of censensus time out cases.
        type: boolean
        default: false
    steps:
      - download-record-streams
      - run-record-stream-validator

  download-record-streams:
    description: Fetch the record files and sigs
    steps:
      - run:
          name: Fetch record stream data from testnet nodes
          command: |
            /repo/.circleci/scripts/trap-failable-for-tf-cleanup.sh \
              '/repo/.circleci/scripts/download_record_stream_data.sh'
  run-record-stream-validator:
    description: Check the record file sigs and running hashes
    parameters:
      perf-run:
        description: If in perf run, the output handling needs to be tolerant of small portion of censensus time out cases.
        type: boolean
        default: false
    steps:
      - run-eet-suites:
          dsl-args: "RecordStreamValidation"
          ci-properties-map: "recordStreamsDir=/repo/recordstreams"

# insight py is removed from services repo and is stages in platform regression repo.
# we no longer use this job for publishing stats.
  publish-platform-stats:
    description: Generate the insight.py PDF and publish it (e.g. to Slack)
    parameters:
      source-desc:
        type: string
        description: Human-readable summary of the source of these stats
        default: 'a variety of test scenarios'
      append-to-last-stats:
        type: string
        description: Append to last-downloaded stats b/c node restarted
        default: 'false'
      publish-to-slack:
        description: publish to slack channel
        type: boolean
        default: true
      workflow-name:
        type: string
        description: which workflow results to report
        default: 'nightly-regression'

    steps:
      - run:
          name: Fetch stats from spot nodes
          command: |
              /repo/.circleci/scripts/collect-node-stats.sh \
                '<< parameters.append-to-last-stats >>'
      - when:
          condition: << parameters.publish-to-slack >>
          steps:
            - run:
                name: Final summary of workflow
                command: |
                  /repo/.circleci/scripts/final-summary.sh \
                      << parameters.workflow-name >>
            - run:
                name: Generate the insight.py PDF
                command: |
                  /repo/.circleci/scripts/trap-failable-for-tf-cleanup.sh \
                     '/repo/.circleci/scripts/gen-stat-insight-pdf.sh'
            - run:
                name: 'Upload the PDF to Slack #hedera-regression channel'
                command: |
                  /repo/.circleci/scripts/publish-stats-pdf-to-slack.sh \
                     "<< parameters.source-desc >>"
  svc-app-slack-msg:
    description: Send a message to Slack via the Services Regression app
    parameters:
      text:
        description: Some literal content to send to Slack.
        type: string
      github-user:
        description: A GitHub user to mention
        type: string
        default: 'nobody-in-particular'
      channel:
        description: A channel ID to target.
        type: string
        default: hedera-regression
      readable:
        description: Human-readable channel description.
        type: string
        default: 'hedera-regression'
    steps:
      - run:
          name: Create a tmp file with the message text.
          command: |
            echo '<< parameters.text >>' > /repo/msg.txt
      - run:
          name: "Say '<< parameters.text >>' to #<< parameters.readable >>"
          command: |
            /repo/.circleci/scripts/call-svcs-app-slack.sh \
                -t /repo/msg.txt \
                -c << parameters.channel >> \
                --github-user << parameters.github-user >>
  svc-app-slack-upload:
    description: Upload a file to Slack via the Services Regression app
    parameters:
      file:
        description: Path of file to upload.
        type: string
      channel:
        description: A channel ID to target.
        type: string
        default: CKWHL8R9A
      readable:
        description: Human-readable channel description.
        type: string
        default: 'hedera-regression'
    steps:
      - run:
          name: "Upload << parameters.file >> to #<< parameters.readable >>"
          command: |
            /repo/.circleci/scripts/call-svcs-app-slack.sh \
                -f << parameters.file >> \
                -c << parameters.channel >>
  cleanup-client-log-storage:
    description: Cleanup test client log storage
    parameters:
      workflow-name:
        description: The current workflow tag
        type: string
        default: 'none-workflow'
    steps:
      - run:
          name: Cleanup old test client log storage
          command: |
            /repo/.circleci/scripts/cleanup-testclient-logs.sh \
               << parameters.workflow-name >>

  save-this-job-client-logs:
    description: Save this client side log files for this job
    parameters:
      workflow-name:
        description: The current workflow name
        type: string
        default: 'none-workflow'
    steps:
      - run:
          name: Save the client side log files for this job
          command: |
            /repo/.circleci/scripts/save-default-client-logs.sh \
               << parameters.workflow-name >>

  fetch-testnet-control-assets:
    description: Add SSH keys, HCL scripts, Ansible playbooks to start testnet
    parameters:
      infra-branch:
        description: Which branch of infrastructure repo to use
        type: string
        default: 'master'
      infra-sha1:
        description: Which commit sha1 of infrastructure repo to use
        type: string
        default: 'HEAD'
    steps:
      - attach_workspace:
          at: /
      - add_ssh_keys:
          fingerprints:
            - "cf:b2:68:a6:65:3f:98:d2:78:18:45:96:b4:fa:b9:1d"
            - "e7:a6:3e:34:3e:d8:fe:64:2c:7f:b6:57:45:03:44:ac"
            - "e7:fd:e2:48:9c:a1:b7:40:95:03:ab:a4:a5:5c:34:21"
      - run:
          name: Checkout infrastructure repo
          command: |
            /repo/.circleci/scripts/trap-failure-report.sh \
              '/repo/.circleci/scripts/clone-infra-repo.sh << parameters.infra-branch >> \
                  << parameters.infra-sha1 >>'
  provision-testnet-hosts:
    description: Provision hosts using Terraform
    parameters:
      num-hosts:
        description: Number of hosts to provision
        type: integer
        default: 4
      liveness-timeout-secs:
        description: Secs to wait for hosts to become available on port 22
        type: integer
        default: 60
      var-file:
        description: Terraform vars to use
        type: string
      var-region:
        description: Configurable region to override pre-defined in terraform config file.
        type: string
        default: 'us-east-1'
      var-ami-id:
        description: ami-id to override pre-defined. Need to be used together with var-region
        type: string
        default: \"\"
    steps:
      - run:
          name: Create Terraform workspace with << parameters.num-hosts >> hosts
          command: |
            /repo/.circleci/scripts/trap-failable-for-tf-cleanup.sh \
                '/repo/.circleci/scripts/create-tf-workspace.sh \
                    << parameters.num-hosts >> \
                    << parameters.liveness-timeout-secs >> \
                    << parameters.var-file >> \
                    << parameters.var-region >> \
                    << parameters.var-ami-id >> '
  pause:
    description: Pause for a given number of seconds
    parameters:
      forSecs:
        type: integer
      reason:
        type: string
        default: ''
    steps:
      - run:
          name: Sleep for << parameters.forSecs >> secs << parameters.reason >>
          command: sleep << parameters.forSecs >>

  prepare-testnet-hosts:
    description: Pickup an existing test net created by terraform through environment varible. No need to pass parameters here.
    steps:
      - run:
          name: Prepare an existing testnet
          command: |
            /repo/.circleci/scripts/trap-failable-for-tf-cleanup.sh \
               '/repo/.circleci/scripts/prepare-testnet.sh'
  deploy-testnet-nodes:
    description: Deploy testnet nodes using Ansible
    parameters:
      liveness-timeout-secs:
        description: Secs to wait for nodes to be reachable
        type: integer
      use-hugepage:
        description: let ansible to deploy in hugepage mode
        type: string
        default: 'false'
    steps:
      - run:
          name: Deploy testnet via Ansible
          command: |
            /repo/.circleci/scripts/trap-failable-for-tf-cleanup.sh \
                '/repo/.circleci/scripts/deploy-testnet.sh \
                     << parameters.liveness-timeout-secs >> << parameters.use-hugepage >>'
  ensure-testnet-for-reruns:
    description: Ensure reruns from failed jobs have a testnet
    steps:
      - run:
          name: Create a job-scoped testnet if none is available
          command: |
            /repo/.circleci/scripts/start-job-scoped-testnet-if-req.sh
      - run:
          name: Import certificates to Java cacerts keystore
          command: |
            /repo/.circleci/scripts/trap-failable-for-tf-cleanup.sh \
               '/repo/.circleci/scripts/import-certificates-to-java-cacerts-keystore.sh'
  cleanup-rerun-testnet-if-present:
    description: Cleanup any testnet created for a rerun job
    steps:
      - run:
          name: Teardown the job-scoped testnet if present
          command: |
            /repo/.circleci/scripts/stop-job-scoped-testnet-if-req.sh
  start-testnet:
    description: Create or use a testnet for the HAPI clients to exercise
    parameters:
      use-existing-network:
        description: tell Circleci to use pre-provisioned testnet instead of creating new a new one.
        type: boolean
        default: false
      var-file:
        description: Terraform vars to use
        type: string
        default: "ci.tfvars"
      infra-branch:
        description: Infrastructure branch to use
        type: string
        default: "hashgraph-hedera-services"
      use-hugepage:
        description: Tell ansible to use hugepage mode when deploying
        type: boolean
        default: false
    steps:
      - fetch-testnet-control-assets:
          infra-branch: << parameters.infra-branch >>
      - unless:
          condition: << parameters.use-existing-network >>
          steps:
            - run: echo "Need to create new spot test net"
            - provision-testnet-hosts:
                liveness-timeout-secs: 120
                var-file: '<< parameters.var-file >>'
                var-region: "us-east-1"
      - when:
          condition: << parameters.use-existing-network >>
          steps:
            - run: echo "Use existing network..."
            - prepare-testnet-hosts
      - run:
          name: Generate self signed certificates for nodes
          command: |
            /repo/.circleci/scripts/generate-self-signed-certificates.sh | \
              tee -a /repo/test-clients/output/hapi-client.log
      - unless:
          condition: << parameters.use-hugepage >>
          steps:
            - deploy-testnet-nodes:
                liveness-timeout-secs: 120
      - when:
          condition: << parameters.use-hugepage >>
          steps:
            - run: echo "Use hugepage mode..."
            - deploy-testnet-nodes:
                liveness-timeout-secs: 120
                use-hugepage: 'true'
      - pause:
          forSecs: 30
      - run:
          name: Disallow any postgres upgrade by apt
          command: |
            /repo/.circleci/scripts/trap-failable-for-tf-cleanup.sh \
                '/repo/.circleci/scripts/disallow-postgres-upgrade.sh'
      - run:
          name: Check logs to see if nodes are brought up with TLS GRPC servers
          command: |
            /repo/.circleci/scripts/show_logs.sh
      - run:
          name: Test TLS connections
          command: |
            /repo/.circleci/scripts/test-tls-connections.sh \
              | tee -a /repo/test-clients/output/hapi-client.log
      - persist_to_workspace:
          root: /
          paths:
            - repo/.circleci
            - repo/certificates
            - infrastructure
  postgres-status:
    description: Summarize PostgreSQL status on the nodes
    steps:
      - run:
          name: Summarize postgres status
          command: |
            /repo/.circleci/scripts/trap-failable-for-tf-cleanup.sh \
                '/repo/.circleci/scripts/check-postgres-status.sh'
  postgres-get-counts:
    description: Get the number of binary objects in the database
    steps:
      - run:
          name: Get Binary Object Counts
          command: |
            /repo/.circleci/scripts/trap-failable-for-tf-cleanup.sh \
                '/repo/.circleci/scripts/postgres-get-counts.sh'
  postgres-verify-counts:
    description: Verify the number of binary objects in the database
    parameters:
      expected-difference:
        description: Expected difference in number of binary objects
        type: integer
    steps:
      - run:
          name: Verify Binary Object Counts
          command: |
            /repo/.circleci/scripts/trap-failable-for-tf-cleanup.sh \
                '/repo/.circleci/scripts/postgres-verify-counts.sh \
                    << parameters.expected-difference >>'
  reboot-testnet:
    description: Reboot testnet nodes using Ansible
    parameters:
      liveness-timeout-secs:
        description: Secs to wait for nodes to be reachable
        type: integer
    steps:
      - run:
          name: Reboot testnet via Ansible
          command: |
            /repo/.circleci/scripts/trap-failable-for-tf-cleanup.sh \
                '/repo/.circleci/scripts/reboot-testnet.sh'
  cleanup-testnet:
    description: Cleanup Terraform resources for the testnet
    parameters:
      var-file:
        description: Terraform vars to use
        type: string
        default: "ci.tfvars"
    steps:
      - run:
          name: Process Terraform workspace at the end
          command: |
            /repo/.circleci/scripts/destroy-tf-workspace.sh \
                '<< parameters.var-file >>'
  check-logs-for-catastrophe:
    description: Scan the logs for any catastrophic failures in Services
    parameters:
      catastrophe-pattern-sh:
        description: Command returning the Services catastrophe pattern
        type: string
        default: /repo/.circleci/scripts/get-catastrophe-pattern.sh
    steps:
      - run:
          name: Scan logs for catastrophic failures
          command: |
            /repo/.circleci/scripts/trap-failable-for-tf-cleanup.sh \
                '/repo/.circleci/scripts/scan-logs-for-catastrophes.sh \
                    << parameters.catastrophe-pattern-sh >>'
  check-logs-for-iss:
    description: Scan the logs for any invalid state signatures
    parameters:
      iss-pattern-sh:
        description: A bash command returning the ISS pattern
        type: string
        default: /repo/.circleci/scripts/get-iss-pattern.sh
    steps:
      - run:
          name: Scan logs for ISS
          command: |
            /repo/.circleci/scripts/trap-failable-for-tf-cleanup.sh \
                '/repo/.circleci/scripts/scan-logs-for-iss.sh \
                    << parameters.iss-pattern-sh >>'

  run-jrs-experiment:
    description: Run JRS experiment json
    parameters:
      experiment-name:
        type: string
        default: ""
    steps:
      - run:
          name: Run experiment << parameters.experiment-name >>.json
          no_output_timeout: 30m
          command: |
            cd /swirlds-platform/regression;
            source /root/.bashrc; source /root/google-cloud-sdk/completion.bash.inc; source /root/google-cloud-sdk/path.bash.inc;
            ./regression_services_circleci.sh configs/services/suites/daily/<< parameters.experiment-name >>.json /repo

  run-eet-suites:
    description: Run end-to-end tests
    parameters:
      node-terraform-index:
        description: Index into array of Terraform-provisioned hosts
        type: integer
        default: 0
      node-account:
        description: Seq num of the Hedera account id for specified node.
        type: integer
        default: 3
      dsl-args:
        description: Args for the EET suite runner main method
        type: string
      ci-properties-map:
        description: Key=value pairs to configure suite behavior
        type: string
        default: ''
      perf-run:
        description: If in perf run, the output handling needs to be tolerant of small portion of censensus time out cases.
        type: boolean
        default: false
    steps:
      - postgres-status
      - unless:
          condition: << parameters.perf-run >>
          steps:
            - run:
                name: Run end-to-end tests '<< parameters.dsl-args >>'
                command: |
                  CI_PROPERTIES_MAP="<< parameters.ci-properties-map >>" \
                  DSL_SUITE_RUNNER_ARGS="<< parameters.dsl-args >>" \
                  /repo/.circleci/scripts/trap-failable-for-tf-cleanup.sh \
                      '/repo/.circleci/scripts/run-scenario-test.sh \
                          com.hedera.services.bdd.suites.SuiteRunner \
                          << parameters.node-terraform-index >> \
                          << parameters.node-account >>'
                no_output_timeout: 60m

      - when:
          condition: << parameters.perf-run >>
          steps:
            - run:
                name: Run end-to-end tests '<< parameters.dsl-args >>'
                command: |
                  CI_PROPERTIES_MAP="<< parameters.ci-properties-map >>" \
                  DSL_SUITE_RUNNER_ARGS="<< parameters.dsl-args >>" \
                  /repo/.circleci/scripts/trap-failable-for-tf-cleanup.sh \
                      '/repo/.circleci/scripts/run-umbrella-redux.sh \
                          com.hedera.services.bdd.suites.SuiteRunner \
                          << parameters.node-terraform-index >> \
                          << parameters.node-account >>'
                no_output_timeout: 60m
      - check-logs-for-iss
  run-property-driven-scenario-test:
    description: |
      Run a self-validating HAPI API scenario that takes non-standard args and
      gets all its host information from the various properties files.
    parameters:
      fqcn:
        description: Fully qualified class name of self-validating test.
        type: string
      args:
        description: Command line args to use.
        type: string
        default: ''
    steps:
      - postgres-status
      - run:
          name: Run self-validating HAPI scenario '<< parameters.fqcn >>'
          command: |
            /repo/.circleci/scripts/trap-failable-for-tf-cleanup.sh \
                '/repo/.circleci/scripts/run-property-driven-scenario-test.sh \
                    << parameters.fqcn >> \
                    << parameters.args >>'
      - check-logs-for-iss
  run-scenario-test:
    description: Run a self-validating HAPI API test scenario.
    parameters:
      fqcn:
        description: Fully qualified class name of self-validating test.
        type: string
      node-terraform-index:
        description: Index into array of Terraform-provisioned hosts.
        type: integer
        default: 0
      node-account:
        description: Seq num of the Hedera account id for specified node.
        type: integer
        default: 3
      other-args:
        description: Any other args consumed by the scenario main method.
        type: string
        default: ''
    steps:
      - postgres-status
      - run:
          name: Run self-validating HAPI scenario '<< parameters.fqcn >>'
          command: |
            /repo/.circleci/scripts/trap-failable-for-tf-cleanup.sh \
                '/repo/.circleci/scripts/run-scenario-test.sh \
                    << parameters.fqcn >> \
                    << parameters.node-terraform-index >> \
                    << parameters.node-account >> \
                    << parameters.other-args >>'
      - check-logs-for-iss
  wait-til-logs-contain:
    description: Wait up to timeout for all host logs to (repeat) a log pattern
    parameters:
      log:
        description: Which log should contain the pattern
        type: string
      pattern-sh:
        description: A bash command returning the pattern to-be-present
        type: string
      pattern-desc:
        description: Human-readable description of the pattern
        type: string
      times:
        description: How many repetitions of the pattern should occur
        type: integer
        default: 1
      timeout-secs:
        description: Timeout for repetitions to be present on all hosts
        type: integer
      sleep-secs:
        description: How long to wait between re-checking for pattern occurrences
        type: integer
    steps:
      - run:
          name: Require << parameters.times >> appearances of << parameters.pattern-desc >> in the << parameters.log >> of all hosts within << parameters.timeout-secs >> secs
          command: |
            /repo/.circleci/scripts/trap-failable-for-tf-cleanup.sh \
                '/repo/.circleci/scripts/wait-til-logs-contain.sh \
                    << parameters.log >> \
                    "<< parameters.pattern-sh >>" \
                    << parameters.times >> \
                    << parameters.timeout-secs >> \
                    << parameters.sleep-secs >>'
  start-freeze:
    description: Freeze the nodes for a short time
    parameters:
      node-terraform-index:
        description: Index into array of Terraform-provisioned hosts.
        type: integer
        default: 0
      node-account:
        description: Seq num of the Hedera account id for specified node.
        type: integer
        default: 3
      failure-log-pattern-sh:
        description: Pattern used to detect a scenario failure in the freeze logs
        type: string
    steps:
      - postgres-status
      - run:
          name: Run 'freeze' test with log-based validation
          command: |
            /repo/.circleci/scripts/trap-failable-for-tf-cleanup.sh \
                '/repo/.circleci/scripts/run-freeze-test.sh \
                    << parameters.node-terraform-index >> \
                    << parameters.node-account >> \
                    << parameters.failure-log-pattern-sh >>'
      - check-logs-for-iss
  validate-thaw:
    description: Complete validation that the nodes were frozen and now thawed
    parameters:
      freeze-start-timeout-secs:
        description: How long til the logs must all have the freeze pattern
        type: integer
        default: 90
      freeze-pattern-count:
        description: How many repetitions of the freeze pattern should occur
        type: integer
        default: 1
    steps:
      - wait-til-logs-contain:
          log: hgcaa.log
          pattern-sh: /repo/.circleci/scripts/get-freeze-pattern.sh
          pattern-desc: freeze pattern
          times: << parameters.freeze-pattern-count >>
          timeout-secs: << parameters.freeze-start-timeout-secs >>
          sleep-secs: 7
      - run:
          name: Validate freeze began in expected window
          command: |
            /repo/.circleci/scripts/trap-failable-for-tf-cleanup.sh \
                '/repo/.circleci/scripts/validate-freeze-start.sh'
      - wait-til-logs-contain:
          log: hgcaa.log
          pattern-sh: /repo/.circleci/scripts/get-thaw-message.sh
          pattern-desc: thaw pattern
          times: 1
          timeout-secs: 180
          sleep-secs: 29
  run-umbrella-test:
    description: Run the so-called 'umbrella' load/longevity test
    parameters:
      config-file:
        description: Properties file to use under test-clients/config/
        type: string
        default: 'umbrellaTest.properties'
      node-terraform-index:
        description: Index into array of Terraform-provisioned hosts.
        type: integer
        default: 0
      node-account:
        description: Seq num of the Hedera account id for specified node.
        type: integer
        default: 3
      expect-unavailable-nodes:
        type: boolean
        description: Flag for whether test is running against frozen nodes
        default: false
    steps:
      - postgres-status
      - run:
          name: Run umbrella test with << parameters.config-file >> <<# parameters.expect-unavailable-nodes >>(PLATFROM should be INACTIVE due to freeze)<</ parameters.expect-unavailable-nodes >>
          command: |
            /repo/.circleci/scripts/trap-failable-for-tf-cleanup.sh \
                '/repo/.circleci/scripts/run-umbrella-test.sh \
                    << parameters.config-file >> \
                    << parameters.node-terraform-index >> \
                    << parameters.node-account >> \
                    << parameters.expect-unavailable-nodes >>'
          no_output_timeout: 90m
      - check-logs-for-iss
  restart-with-validations:
    description: Restart the testnet, validate node logs
    parameters:
      active-status-timeout-secs:
        description: How long til nodes must come active
        type: integer
        default: 30
      valid-ss-timeout-secs:
        description: How long til nodes must confirm a valid signed state
        type: integer
        default: 120
      liveness-pattern-count:
        description: How many repetitions of the liveness pattern should occur
        type: integer
        default: 3
      signed-state-pattern-count:
        description: How many repetitions of the signed state pattern should occur
        type: integer
        default: 1
    steps:
      - postgres-status
      - reboot-testnet:
          liveness-timeout-secs: 60
      - pause:
          forSecs: 30
      - wait-til-logs-contain:
          log: hgcaa*.log
          pattern-sh: /repo/.circleci/scripts/get-liveness-pattern.sh
          pattern-desc: alive pattern
          times: << parameters.liveness-pattern-count >>
          timeout-secs: << parameters.active-status-timeout-secs >>
          sleep-secs: 7
      - wait-til-logs-contain:
          log: hgcaa*.log
          pattern-sh: /repo/.circleci/scripts/get-signed-state-pattern.sh
          pattern-desc: merkle state restored
          times: << parameters.signed-state-pattern-count >>
          timeout-secs: << parameters.valid-ss-timeout-secs >>
          sleep-secs: 7
      - wait-til-logs-contain:
          log: swirlds.log
          pattern-sh: /repo/.circleci/scripts/get-lateseq-pattern.sh
          pattern-desc: last known sequence numbers after restart pattern
          times: << parameters.signed-state-pattern-count >>
          timeout-secs: 60
          sleep-secs: 7
      - run:
          name: Scan log of node 0.0.3 for restart lateseq
          command: |
            /repo/.circleci/scripts/trap-failable-for-tf-cleanup.sh \
                '/repo/.circleci/scripts/await-default-node-lateseq.sh \
                    /repo/.circleci/scripts/get-lateseq-pattern.sh \
                    60 \
                    7'
      - wait-til-logs-contain:
          log: swirlds.log
          pattern-sh: /repo/.circleci/scripts/get-first-lateseq.sh
          pattern-desc: the same lateseq
          times: 1
          timeout-secs: 30
          sleep-secs: 7
      - check-logs-for-iss

  accessory-test-common-steps:
    description: shared steps for accessory tests
    steps:
      - run-eet-suites:
          dsl-args: "CryptoTransferSuite"
      - run-eet-suites:
          dsl-args: "ContractCallLocalSuite -TLS=on"
      - run-eet-suites:
          dsl-args: "ContractCallSuite -TLS=on"
      - run-eet-suites:
          dsl-args: "ChildStorageSpecs -TLS=on"
      - run-eet-suites:
          dsl-args: "BigArraySpec -TLS=on"
      - run-eet-suites:
          dsl-args: "SmartContractInlineAssemblySpec -TLS=on"
      - run-eet-suites:
          dsl-args: "OCTokenSpec -TLS=on"
      - run-eet-suites:
          dsl-args: "CharacterizationSuite -TLS=on"
      - run-eet-suites:
          dsl-args: "SmartContractFailFirstSpec -TLS=on"
      - run-eet-suites:
          dsl-args: "SmartContractSelfDestructSpec -TLS=on"
      - run-eet-suites:
          dsl-args: "CryptoTransferSuite"
      - run-eet-suites:
          dsl-args: "CryptoQueriesStressTests"
      - run-eet-suites:
          dsl-args: "FileQueriesStressTests"
      - run-eet-suites:
          dsl-args: "ConsensusQueriesStressTests"
      - run-eet-suites:
          dsl-args: "ContractQueriesStressTests"
      - run:
          name: Set log level to INFO
          command: /repo/.circleci/scripts/config-log4j-4normal.sh

executors:
  build-executor:
    resource_class: xlarge
    parameters:
      workflow-name:
        type: string
        default: ""
    docker:
      - image: qnswirlds/java-builder:0.1.3
      - image: postgres:10
        environment:
          POSTGRES_USER: swirlds
          POSTGRES_PASSWORD: password
          POSTGRES_DB: fcfs
    environment:
      MAVEN_OPTS: -Xmx3200m
      IN_CIRCLE_CI: true
      REPO: /repo
      WORKFLOW-NAME: << parameters.workflow-name >>
    working_directory: /repo

  ci-test-executor:
    parameters:
      tf_workspace:
        type: string
        default: ""
      tf_dir:
        type: string
        default: "/infrastructure/terraform/deployments/aws-4-node-spot-net-swirlds"
      use_existing_network:
        type: string
        default: ""
      workflow-name:
        type: string
        default: ""
    docker:
      - image: qnswirlds/java-builder:0.1.3
    environment:
      TF_DIR: << parameters.tf_dir >>
      IN_CIRCLE_CI: true
      USE_EXISTING_NETWORK: <<parameters.use_existing_network>>
      TF_WORKSPACE: << parameters.tf_workspace >>
      REPO: /repo
      INFRASTRUCTURE_REPO: /infrastructure
      WORKFLOW-NAME: << parameters.workflow-name >>
    working_directory: /repo

workflows:
  GCP-Daily-Services-Crypto-Update-5N-1C:
    triggers:
      - schedule:
          cron: "45 16 * * *"
          filters:
            branches:
              only:
                - 1035-M-Change-log4j-format
    jobs:
      - build-platform-and-services
      - jrs-regression:
          context: Slack
          result_path: results/5N_1C/Update
          config_type: "daily"
          workflow-name: "GCP-Daily-Services-Crypto-Update-5N-1C"
          requires:
            - build-platform-and-services
          pre-steps:
            - install-tools
            - attach_workspace:
                at: /

  GCP-Weekly-Services-Crypto-Restart-Performance-15N-15C:
    triggers:
      - schedule:
          cron: "5 5 * * 6,0"
          filters:
            branches:
              only:
                - master
    jobs:
      - build-platform-and-services
      - jrs-regression:
          context: Slack
          regression_path: /swirlds-platform/regression
          result_path: results/15N_15C/
          config_type: "weekly"
          workflow-name: "GCP-Weekly-Services-Crypto-Restart-Performance-15N-15C"
          requires:
            - build-platform-and-services
          pre-steps:
            - install-tools
            - attach_workspace:
                at: /

  GCP-Weekly-Services-HCS-Restart-Performance-15N-15C:
    triggers:
      - schedule:
          cron: "5 10 * * 6,0"
          filters:
            branches:
              only:
                - master
    jobs:
      - build-platform-and-services
      - jrs-regression:
          context: Slack
          regression_path: /swirlds-platform/regression
          result_path: results/15N_15C/
          config_type: "weekly"
          workflow-name: "GCP-Weekly-Services-HCS-Restart-Performance-15N-15C"
          requires:
            - build-platform-and-services
          pre-steps:
            - install-tools
            - attach_workspace:
                at: /

  GCP-Weekly-Services-HTS-Restart-Performance-15N-15C:
    triggers:
      - schedule:
          cron: "5 15 * * 6,0"
          filters:
            branches:
              only:
                - master
    jobs:
      - build-platform-and-services
      - jrs-regression:
          context: Slack
          regression_path: /swirlds-platform/regression
          result_path: results/15N_15C/
          config_type: "weekly"
          workflow-name: "GCP-Weekly-Services-HTS-Restart-Performance-15N-15C"
          requires:
            - build-platform-and-services
          pre-steps:
            - install-tools
            - attach_workspace:
                at: /

  GCP-Daily-Services-Comp-NetError-4N-1C:
    triggers:
      - schedule:
          cron: "20 5 * * *"
          filters:
            branches:
              only:
                - master
    jobs:
      - build-platform-and-services
      - jrs-regression:
          context: Slack
          regression_path: /swirlds-platform/regression
          result_path: results/4N_1C/NetError
          config_type: "daily"
          workflow-name: "GCP-Daily-Services-Comp-NetError-4N-1C"
          requires:
            - build-platform-and-services
          pre-steps:
            - install-tools
            - attach_workspace:
                at: /

  # This workflow will be running normally in master branch on a daily basis

  GCP-Daily-Services-Crypto-Migration-5N-1C:
    triggers:
      - schedule:
          cron: "45 16 * * *"
          filters:
            branches:
              only:
                - 1035-M-Change-log4j-format
    jobs:
      - build-platform-and-services
      - update-start-up-key:
          requires:
            - build-platform-and-services
      - jrs-regression:
          context: Slack
          regression_path: /swirlds-platform/regression
          result_path: results/5N_1C/Migration
          config_type: "daily"
          workflow-name: "GCP-Daily-Services-Crypto-Migration-5N-1C"
          requires:
            - update-start-up-key
          pre-steps:
            - install-tools
            - attach_workspace:
                at: /

# The mainnet migration test needs to be run in a 13-node test network.
# Enable this workflow to run on a daily only when a new release branch is tagged and branched.
# Once this branch is released, disable this workflow.
  nightly-mainnet-migration-regression:
    triggers:
          - schedule:
              cron: "0 23,8 * * *"
              filters:
                branches:
                  only:
                    - release-branch-N
    jobs:
      - build-platform-and-services
      - run-mainnet-migration-regression:
          context: Slack
          requires:
            - build-platform-and-services
          pre-steps:
            - install-tools
            - attach_workspace:
                at: /
          workflow-name: "nightly-mainnet-migration-regression"

  GCP-Daily-Services-Crypto-Restart-4N-1C:
    triggers:
      - schedule:
          cron: "5 6 * * *"
          filters:
            branches:
              only:
                - master
    jobs:
      - build-platform-and-services
      - jrs-regression:
          context: Slack
          regression_path: /swirlds-platform/regression
          result_path: results/4N_1C/Restart
          config_type: "daily"
          workflow-name: "GCP-Daily-Services-Crypto-Restart-4N-1C"
          requires:
            - build-platform-and-services
          pre-steps:
            - install-tools
            - attach_workspace:
                at: /

  GCP-Daily-Services-Recovery-4N-1C:
    triggers:
      - schedule:
          cron: "35 6 * * *"
          filters:
            branches:
              only:
                - master
    jobs:
      - build-platform-and-services
      - jrs-regression:
          context: Slack
          regression_path: /swirlds-platform/regression
          result_path: results/4N_1C/Recovery
          config_type: "daily"
          workflow-name: "GCP-Daily-Services-Recovery-4N-1C"
          requires:
            - build-platform-and-services
          pre-steps:
            - install-tools
            - attach_workspace:
                at: /

  GCP-Daily-Services-Comp-Restart-Performance-Hotspot-6N-6C:
    triggers:
      - schedule:
          cron: "35 6 * * *"
          filters:
            branches:
              only:
                - master
    jobs:
      - build-platform-and-services
      - jrs-regression:
          context: Slack
          regression_path: /swirlds-platform/regression
          result_path: results/6N_6C/Performance
          config_type: "daily"
          workflow-name: "GCP-Daily-Services-Comp-Restart-Performance-Hotspot-6N-6C"
          requires:
            - build-platform-and-services
          pre-steps:
            - install-tools
            - attach_workspace:
                at: /

  GCP-Daily-Services-Comp-Restart-Performance-Random-6N-6C:
    triggers:
      - schedule:
          cron: "30 4 * * *"
          filters:
            branches:
              only:
                - master
    jobs:
      - build-platform-and-services
      - jrs-regression:
          context: Slack
          regression_path: /swirlds-platform/regression
          result_path: results/6N_6C/Performance
          config_type: "daily"
          workflow-name: "GCP-Daily-Services-Comp-Restart-Performance-Random-6N-6C"
          requires:
            - build-platform-and-services
          pre-steps:
            - install-tools
            - attach_workspace:
                at: /

  GCP-Daily-Services-Comp-Reconnect-4N-1C:
    triggers:
      - schedule:
          cron: "5 7 * * *"
          filters:
            branches:
              only:
                - master
    jobs:
      - build-platform-and-services
      - jrs-regression:
          context: Slack
          regression_path: /swirlds-platform/regression
          result_path: results/4N_1C/Reconnect
          config_type: "daily"
          workflow-name: "GCP-Daily-Services-Comp-Reconnect-4N-1C"
          requires:
            - build-platform-and-services
          pre-steps:
            - install-tools
            - attach_workspace:
                at: /

  nightly-simulate-network-outage:
    triggers:
        - schedule:
            cron: "0 6 * * *"
            filters:
              branches:
                only:
                  - master
    jobs:
      - fast-build-artifact:
          context: Slack
          workflow-name: "nightly-simulate-network-outage"
      - start-singlejob-testnet:
          context: Slack
          requires:
            - fast-build-artifact
          workflow-name: "nightly-simulate-network-outage"
          infra-branch: hashgraph-hedera-services
      - run-accessory-tests:
          context: Slack
          requires:
            - start-singlejob-testnet
          pre-steps:
            - attach_workspace:
                at: /
            - run: /repo/.circleci/scripts/echo-env.sh
            - ensure-testnet-for-reruns
          post-steps:
            - cleanup-rerun-testnet-if-present
          workflow-name: "nightly-simulate-network-outage"
      - run-network-sim:
          context: Slack
          requires:
            - run-accessory-tests
          pre-steps:
            - attach_workspace:
                at: /
            - run: /repo/.circleci/scripts/echo-env.sh
            - ensure-testnet-for-reruns
          post-steps:
            - cleanup-rerun-testnet-if-present
          workflow-name: "nightly-simulate-network-outage"
      - rerun-accessory-tests:
          context: Slack
          requires:
            - run-network-sim
          pre-steps:
            - attach_workspace:
                at: /
            - run: /repo/.circleci/scripts/echo-env.sh
            - ensure-testnet-for-reruns
          post-steps:
            - send-report-to-slack:
                workflow-name: simulate network outage
                status: Passed
            - cleanup-rerun-testnet-if-present
          workflow-name: "nightly-simulate-network-outage"
      - cleanup-singlejob-testnet:
          context: Slack
          requires:
            - rerun-accessory-tests
          pre-steps:
            - attach_workspace:
                at: /

  feature-update-regression:
    triggers:
        - schedule:
            cron: "0 8 * * *"
            filters:
              branches:
                only:
                  - master
    jobs:
      - fast-build-artifact:
          context: Slack
          post-steps:
            - save-this-job-client-logs:
                workflow-name: "feature-update-regression"
          workflow-name: "feature-update-regression"
      - start-singlejob-testnet:
          context: Slack
          requires:
            - fast-build-artifact
          workflow-name: "feature-update-regression"
          infra-branch: hashgraph-hedera-services
          post-steps:
            - save-this-job-client-logs:
                workflow-name: "feature-update-regression"
      - run-accessory-tests:
          context: Slack
          requires:
            - start-singlejob-testnet
          post-steps:
            - save-this-job-client-logs:
                workflow-name: "feature-update-regression"
            - cleanup-rerun-testnet-if-present
          workflow-name: "feature-update-regression"
      - run-update-feature:
          context: Slack
          requires:
            - run-accessory-tests
          post-steps:
            - save-this-job-client-logs:
                workflow-name: "feature-update-regression"
            - cleanup-rerun-testnet-if-present
          workflow-name: "feature-update-regression"
      - run-update-jar-files:
          context: Slack
          requires:
            - run-update-feature
          post-steps:
            - save-this-job-client-logs:
                workflow-name: "feature-update-regression"
            - cleanup-rerun-testnet-if-present
          new-marker : "new marker string"
          workflow-name: "feature-update-regression"
      - rerun-accessory-tests:
          context: Slack
          requires:
            - run-update-jar-files
          post-steps:
            - save-this-job-client-logs:
                workflow-name: "feature-update-regression"
            - validate-feature-update-test-report:
                status: "Passed"
                workflow-name: "feature-update-regression"
            - cleanup-rerun-testnet-if-present
          workflow-name: "feature-update-regression"
      - cleanup-singlejob-testnet:
          context: Slack
          requires:
            - rerun-accessory-tests
          pre-steps:
            - attach_workspace:
                at: /

  nightly-freeze-restart:
    triggers:
      - schedule:
          cron: "0 5 * * *"
          filters:
            branches:
              only:
                - master
    jobs:
      - fast-build-artifact:
          context: Slack
          workflow-name: freeze restart
      - start-singlejob-testnet:
          context: Slack
          requires:
            - fast-build-artifact
          workflow-name: freeze restart
          infra-branch: hashgraph-hedera-services
      - run-umbrella-freeze-restart-test:
          context: Slack
          name: freeze-restart-first-round
          requires:
            - start-singlejob-testnet
          pre-steps:
            - attach_workspace:
                at: /
            - run: /repo/.circleci/scripts/echo-env.sh
            - ensure-testnet-for-reruns
          post-steps:
            - cleanup-rerun-testnet-if-present
          freeze-pattern-count: 1
          liveness-pattern-count: 3
          signed-state-pattern-count: 1
          workflow-name: freeze restart
      - run-umbrella-freeze-restart-test:
          context: Slack
          name: freeze-restart-second-round
          requires:
            - freeze-restart-first-round
          pre-steps:
            - attach_workspace:
                at: /
            - run: /repo/.circleci/scripts/echo-env.sh
            - ensure-testnet-for-reruns
          post-steps:
            - cleanup-rerun-testnet-if-present
          freeze-pattern-count: 2
          liveness-pattern-count: 5
          signed-state-pattern-count: 2
          workflow-name: freeze restart
      - run-umbrella-freeze-restart-test:
          context: Slack
          name: freeze-restart-third-round
          requires:
            - freeze-restart-second-round
          pre-steps:
            - attach_workspace:
                at: /
            - run: /repo/.circleci/scripts/echo-env.sh
            - ensure-testnet-for-reruns
          post-steps:
            - publish-platform-stats:
                source-desc: freeze restart
            - cleanup-rerun-testnet-if-present
          freeze-pattern-count: 3
          liveness-pattern-count: 7
          signed-state-pattern-count: 3
          workflow-name: freeze restart
      - cleanup-singlejob-testnet:
          context: Slack
          requires:
            - freeze-restart-third-round
          pre-steps:
            - attach_workspace:
                at: /

  daily-aws-summary:
    triggers:
      - schedule:
          cron: "15 14 * * *"
          filters:
            branches:
              only:
                - master
    jobs:
      - aws-summary:
          context: Slack

  nightly-comprehensive-functional-regression:
    triggers:
      - schedule:
          cron: "15 4 * * *"
          filters:
            branches:
              only:
                - master
    jobs:
      - fast-build-artifact:
          context: Slack
          workflow-name: "nightly-comprehensive-functional-regression"
      - start-singlejob-testnet:
          context: Slack
          requires:
            - fast-build-artifact
          workflow-name: "nightly-comprehensive-functional-regression"
      - run-comprehensive-functional-test:
          context: Slack
          requires:
            - start-singlejob-testnet
          pre-steps:
            - attach_workspace:
                at: /
            - ensure-testnet-for-reruns
          post-steps:
            - cleanup-rerun-testnet-if-present
          workflow-name: "nightly-comprehensive-functional-regression"

      - cleanup-singlejob-testnet:
          context: Slack
          requires:
            - run-comprehensive-functional-test
          pre-steps:
            - attach_workspace:
                at: /

  continuous-integration-gcp:
      jobs:
        - build-artifact:
            context: SonarCloud
            filters:
              branches:
                ignore:
                  - /.*-PERF/
            workflow-name: "Continuous integration"
        - build-platform-and-services:
            context: SonarCloud
            requires:
              - build-artifact
            workflow-name: "Continuous integration GCP"
        - run-continuous-integration-gcp:
            context: Slack
            requires:
              - build-platform-and-services
            pre-steps:
              - install-tools
              - attach_workspace:
                  at: /
            workflow-name: "Continuous Integration GCP"

  continuous-integration:
    jobs:
      - build-artifact:
          context: SonarCloud
          filters:
            branches:
              ignore:
                - /.*-PERF/
                - /.*-REGRESSION/

          workflow-name: "Continuous integration"
      - start-singlejob-testnet:
          context: Slack
          requires:
            - build-artifact
          workflow-name: "Continuous integration"
          infra-branch: hashgraph-hedera-services
      - run-schedule-scenarios:
          context: Slack
          requires:
            - start-singlejob-testnet
          pre-steps:
            - attach_workspace:
                at: /
            - ensure-testnet-for-reruns
          post-steps:
            - cleanup-rerun-testnet-if-present
          workflow-name: "Continuous integration"
      - run-token-scenarios:
          context: Slack
          requires:
            - run-schedule-scenarios
          pre-steps:
            - attach_workspace:
                at: /
            - ensure-testnet-for-reruns
          post-steps:
            - cleanup-rerun-testnet-if-present
          workflow-name: "Continuous integration"
      - run-file-scenarios:
          context: Slack
          requires:
            - run-token-scenarios
          pre-steps:
            - attach_workspace:
                at: /
            - ensure-testnet-for-reruns
          post-steps:
            - cleanup-rerun-testnet-if-present
          workflow-name: "Continuous integration"
      - run-consensus-and-crypto-scenarios:
          context: Slack
          requires:
            - run-file-scenarios
          pre-steps:
            - attach_workspace:
                at: /
            - run: /repo/.circleci/scripts/echo-env.sh
            - ensure-testnet-for-reruns
          post-steps:
            - cleanup-rerun-testnet-if-present
          workflow-name: "Continuous integration"
      - run-smart-contract-scenarios:
          context: Slack
          requires:
            - run-consensus-and-crypto-scenarios
          pre-steps:
            - attach_workspace:
                at: /
            - ensure-testnet-for-reruns
          post-steps:
            - send-report-to-slack:
                workflow-name: "Continuous integration"
                status: Passed
            - cleanup-rerun-testnet-if-present
          workflow-name: "Continuous integration"
      - cleanup-singlejob-testnet:
          context: Slack
          requires:
            - run-smart-contract-scenarios
          pre-steps:
            - attach_workspace:
                at: /
jobs:
  start-singlejob-testnet:
    parameters:
      infra-branch:
        description: Infrastructure branch to use
        type: string
        default: "hashgraph-hedera-services"
      workflow-name:
        type: string
        default: ""
    executor:
      name: ci-test-executor
      workflow-name:  << parameters.workflow-name >>
    steps:
      - start-testnet:
          infra-branch: << parameters.infra-branch >>
  cleanup-singlejob-testnet:
    executor: ci-test-executor
    steps:
      - cleanup-testnet
  start-multijob-testnet:
    executor: ci-test-executor
    steps:
      - attach_workspace:
          at: /
      - start-testnet:
          var-file: "regression.tfvars"
      - persist_to_workspace:
          root: /
          paths:
            - repo/.circleci
            - infrastructure

  build-artifact:
    parameters:
      sys-props:
        type: string
        description: System properties for mvn install
        default: ''
      mvn-args:
        type: string
        description: Args for mvn install
        default: ''
      workflow-name:
        type: string
        default: ""
    executor:
      name: build-executor
      workflow-name:  << parameters.workflow-name >>
    steps:
      - checkout
      - run:
          name: prepare log dir
          command: |
            mkdir -p /repo/test-clients/output
      - run:
          name: mvn install
          # use double quote otherwise the backslash of line continuation will be treated as part of mvn parameter
          command: |
            /repo/.circleci/scripts/trap-failure-report.sh \
              "mvn --no-transfer-progress -Dsonar.branch.name=${CIRCLE_BRANCH} -Dsonar.login=${SONAR_TOKEN} clean install sonar:sonar \
              | tee /repo/test-clients/output/hapi-client.log"
      - run:
          name: Save Unit Test Results
          command: |
            set -x
            JUNIT_PATH="${HOME}/junit"

            if [[ ! -d "${JUNIT_PATH}" ]]; then
              mkdir -pv "${JUNIT_PATH}"
            fi
            find . -type f -regex ".*/target/surefire-reports/.*xml" -exec cp -v {} "${JUNIT_PATH}" \;
          when: always
      - store_test_results:
          path: ~/junit
      - store_artifacts:
          path: ~/junit
      - run:
          name: Upload codecov
          command: |
            apt-get update
            apt-get install -y curl
            bash <(wget -O - https://codecov.io/bash)
      - run:
          name: Upload built artifacts to S3 (for Ansible download)
          command: |
            /repo/.circleci/scripts/trap-failure-report.sh \
               '/repo/.circleci/scripts/package-hapi-artifacts.sh hedera-node/data/lib'
            /repo/.circleci/scripts/trap-failure-report.sh \
               '/repo/.circleci/scripts/upload-dir-to-s3.sh \
                hedera-node/data lib SHA-${CIRCLE_SHA1}'
      - run:
          name: Reposition config for Ansible scripts
          command: |
            /repo/.circleci/scripts/trap-failure-report.sh \
                'mkdir -p /repo/HapiApp2.0'
            /repo/.circleci/scripts/trap-failure-report.sh \
                'cp /repo/hedera-node/log4j2.xml /repo/HapiApp2.0'
      - persist_to_workspace:
          root: /
          paths:
            - repo/.git
            - repo/.circleci
            - repo/hapiProto
            - repo/test-clients
            - repo/HapiApp2.0
            - root/.m2
            - repo/hedera-node

  fast-build-artifact:
    parameters:
      workflow-name:
        type: string
        default: ""
    executor:
      name: build-executor
      workflow-name:  << parameters.workflow-name >>
    steps:
      - checkout
      - cleanup-client-log-storage:
          workflow-name: << parameters.workflow-name >>
      - run:
          name: prepare log dir
          command: |
            mkdir -p /repo/test-clients/output
      - run:
          name: mvn install
          command: |
            /repo/.circleci/scripts/trap-failure-report.sh \
               'mvn --no-transfer-progress clean install -DskipTests | tee /repo/test-clients/output/hapi-client.log'
      - run:
          name: Upload built artifacts to S3 (for Ansible download)
          command: |
            /repo/.circleci/scripts/trap-failure-report.sh \
              '/repo/.circleci/scripts/package-hapi-artifacts.sh \
                     hedera-node/data/lib | tee -a /repo/test-clients/output/hapi-client.log'
            /repo/.circleci/scripts/trap-failure-report.sh \
              '/repo/.circleci/scripts/upload-dir-to-s3.sh \
                     hedera-node/data lib SHA-${CIRCLE_SHA1} | tee -a /repo/test-clients/output/hapi-client.log'
      - run:
          name: Reposition config for Ansible scripts
          command: |
            /repo/.circleci/scripts/trap-failure-report.sh \
               'mkdir -p /repo/HapiApp2.0'
            /repo/.circleci/scripts/trap-failure-report.sh \
               'cp /repo/hedera-node/log4j2.xml /repo/HapiApp2.0'
      - persist_to_workspace:
          root: /
          paths:
            - repo/.git
            - repo/.circleci
            - repo/hapiProto
            - repo/test-clients
            - repo/HapiApp2.0
            - root/.m2
            - repo/hedera-node

  run-mainnet-migration-regression:
    parameters:
      workflow-name:
        type: string
        default: ""
    executor:
      name: build-executor
      workflow-name:  << parameters.workflow-name >>
    steps:
      - run:
          name: Modify Payer account for Mainnet Migration test
          command: |
            sed -i 's/default.payer=0.0.2/default.payer=0.0.950/g' /repo/test-clients/src/main/resource/spec-default.properties;
            echo -n "$KEY_950" > /repo/test-clients/src/main/resource/StartUpAccount.txt;

      - run:
          name: Run Mainnet Migration test
          command: |
            cd /repo ; mvn --no-transfer-progress clean install -DskipTests;
            cd /swirlds-platform/regression;
            ./regression_services_circleci.sh configs/services/daily/13N_13C/AWS-Services-Daily-MainnetMigration-13N-1C.json /repo

      - run:
          name: Sync results of mainnet migration test to AWS
          command: |
            cd /swirlds-platform/regression/results/; find . -type d -name data -prune -exec rm -rf {} \;
            aws s3 sync /swirlds-platform/regression/results/ s3://hedera-service-regression-jrs;
            tar -czvf /results.tar.gz  /swirlds-platform/regression/results/*

      - store_artifacts:
          path: /results.tar.gz

  update-start-up-key:
    parameters:
      workflow-name:
        type: string
        default: ""
    executor:
      name: build-executor
    steps:
      - attach_workspace:
          at: /
      - run:
          name: Modify Payer account for Public testnet start from saved state test
          command: |
            echo -n "$KEY_DevTestNetTreasury" > /repo/test-clients/src/main/resource/StartUpAccount.txt;
      - persist_to_workspace:
          root: /
          paths:
            - repo/
            - swirlds-platform/

  run-continuous-integration-gcp:
    parameters:
      workflow-name:
        type: string
        default: ""
    executor:
      name: build-executor
      workflow-name: << parameters.workflow-name >>
    steps:
      - run:
          name: Run continuous integration tests
          no_output_timeout: 30m
          command: |
            cd /swirlds-platform/regression;
            source /root/.bashrc; source /root/google-cloud-sdk/completion.bash.inc; source /root/google-cloud-sdk/path.bash.inc;
            ./regression_services_circleci.sh configs/services/suites/ci/GCP-Commit-Services-Comp-Basic-4N-1C.json /repo $CIRCLE_USERNAME $CIRCLE_BUILD_URL

      - run:
          name: Sync results of continuous integration to circelCI job
          command: |
            tar -czvf /results.tar.gz  /swirlds-platform/regression/results/4N_1C/CI/*

      - store_artifacts:
          path: /results.tar.gz

  build-platform-and-services:
    parameters:
      workflow-name:
        type: string
        default: ""
    executor:
      name: build-executor
    steps:
      - attach_workspace:
          at: /
      - add_ssh_keys:
          fingerprints:
            - "96:47:c4:5c:e7:45:06:c5:26:a5:85:ef:41:22:2f:d6"
            - "14:21:e9:81:1f:ae:df:ec:11:60:4a:49:e0:b9:bb:58"
            - "e7:a6:3e:34:3e:d8:fe:64:2c:7f:b6:57:45:03:44:ac"
      - checkout
      - run:
          name: Build hedera-services repo
          command: |
                mvn --no-transfer-progress clean install -DskipTests
      - run:
          name: Checkout swirlds-platform repos and build
          command: |
            sed -i -e 's/Host services-jrs-regression/Host services-jrs-regression\n HostName github.com/g' ~/.ssh/config
            cd /; GIT_SSH_COMMAND="ssh -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no" \
            git clone ssh://git@services-jrs-regression/swirlds/swirlds-platform.git;
            cd /swirlds-platform;
            sed -i -e 's/github.com/services-jrs-regression/g' .gitmodules;
            git submodule update --init --recursive --checkout;
            cd regression; git checkout 00865-D-update-log4j-pattern;
            cd ..;
            mvn --no-transfer-progress clean install -DskipTests;

      - run:
          name: Save PEM file to keys folder
          command: |
            cp ~/.ssh/id_rsa_e7a63e343ed8fe642c7fb657450344ac /swirlds-platform/regression/keys/services-regression.pem

      - persist_to_workspace:
          root: /
          paths:
            - repo/
            - swirlds-platform/

  aws-summary:
    executor: ci-test-executor
    steps:
      - checkout
      - run:
          name: Summarize all AWS instances currently used by regions
          command: |
            /repo/.circleci/scripts/aws-summary.sh

  run-accessory-tests:
    parameters:
      runner:
        type: executor
        default: ci-test-executor
      workflow-name:
        type: string
        default: ""
    executor:
      name: ci-test-executor
      workflow-name:  << parameters.workflow-name >>
    steps:
      - attach_workspace:
          at: /
      - ensure-testnet-for-reruns
      - run:
          name:  Reset client log
          command: |
            cat /dev/null > /repo/test-clients/output/hapi-client.log
      - run: /repo/.circleci/scripts/echo-env.sh
      - accessory-test-common-steps

  rerun-accessory-tests:
    parameters:
      runner:
        type: executor
        default: ci-test-executor
      workflow-name:
        type: string
        default: ""
    executor:
      name: ci-test-executor
      workflow-name:  << parameters.workflow-name >>
    steps:
      - attach_workspace:
          at: /
      - ensure-testnet-for-reruns
      - run:
          name:  Reset client log
          command: |
            cat /dev/null > /repo/test-clients/output/hapi-client.log
      - run: /repo/.circleci/scripts/echo-env.sh
      - accessory-test-common-steps

  run-consensus-and-crypto-scenarios:
    parameters:
      runner:
        type: executor
        default: ci-test-executor
      workflow-name:
        type: string
        default: ""
    executor:
      name: ci-test-executor
      workflow-name:  << parameters.workflow-name >>
    steps:
      - run-eet-suites:
          dsl-args: "SignedTransactionBytesRecordsSuite"
      - run-eet-suites:
          dsl-args: "TopicCreateSpecs"
      - run-eet-suites:
          dsl-args: "TopicUpdateSpecs -TLS=on"
          node-terraform-index: 1
          node-account: 4
      - run-eet-suites:
          dsl-args: "TopicDeleteSpecs"
      - run-eet-suites:
          dsl-args: "SubmitMessageSpecs"
      - run-eet-suites:
          dsl-args: "HCSTopicFragmentationSuite -TLS=alternate"
      - run-eet-suites:
          dsl-args: "TopicGetInfoSpecs"
      - run-eet-suites:
          dsl-args: "BucketThrottlingSpec"
      - run-eet-suites:
          dsl-args: "ControlAccountsExemptForUpdates"
      - run-eet-suites:
          dsl-args: "CryptoTransferSuite"
      - run-eet-suites:
          dsl-args: "CryptoUpdateSuite -TLS=on"
      - run-eet-suites:
          dsl-args: "CryptoTransferSuite"
      - run-eet-suites:
          dsl-args: "CryptoRecordSanityChecks"
      - run-eet-suites:
          dsl-args: "SuperusersAreNeverThrottled"
      - run-eet-suites:
          dsl-args: "CannotDeleteSystemEntitiesSuite"

  run-schedule-scenarios:
    parameters:
      runner:
        type: executor
        default: ci-test-executor
      workflow-name:
        type: string
        default: ""
    executor:
      name: ci-test-executor
      workflow-name:  << parameters.workflow-name >>
    steps:
      - run-eet-suites:
          dsl-args: "ScheduleDeleteSpecs"
      - run-eet-suites:
          dsl-args: "ScheduleExecutionSpecs"
      - run-eet-suites:
          dsl-args: "ScheduleCreateSpecs"
      - run-eet-suites:
          dsl-args: "ScheduleSignSpecs"
      - run-eet-suites:
          dsl-args: "ScheduleRecordSpecs"

  run-token-scenarios:
    parameters:
      runner:
        type: executor
        default: ci-test-executor
      workflow-name:
        type: string
        default: ""
    executor:
      name: ci-test-executor
      workflow-name:  << parameters.workflow-name >>
    steps:
      - run-eet-suites:
          dsl-args: "TokenAssociationSpecs"
      - run-eet-suites:
          dsl-args: "TokenCreateSpecs"
      - run-eet-suites:
          dsl-args: "TokenUpdateSpecs"
      - run-eet-suites:
          dsl-args: "TokenDeleteSpecs"
      - run-eet-suites:
          dsl-args: "TokenManagementSpecs"
      - run-eet-suites:
          dsl-args: "TokenTransactSpecs"

  run-file-scenarios:
    parameters:
      runner:
        type: executor
        default: ci-test-executor
      workflow-name:
        type: string
        default: ""
    executor:
      name: ci-test-executor
      workflow-name:  << parameters.workflow-name >>
    steps:
      - run-eet-suites:
          dsl-args: "FileCreateSuite"
      - run-eet-suites:
          dsl-args: "FileAppendSuite"
      - run-eet-suites:
          dsl-args: "FileUpdateSuite"
      - run-eet-suites:
          dsl-args: "ProtectedFilesUpdateSuite"
      - run-eet-suites:
          dsl-args: "PermissionSemanticsSpec"
      - run-eet-suites:
          dsl-args: "ExchangeRateControlSuite"
      - run-eet-suites:
          dsl-args: "SysDelSysUndelSpec"
      - run-eet-suites:
          dsl-args: "ExchangeRateControlSuite"
      - run-eet-suites:
          dsl-args: "UpdateFailuresSpec"
      - run-eet-suites:
          dsl-args: "QueryFailuresSpec"
      - run-eet-suites:
          dsl-args: "FileRecordSanityChecks"
      - run-eet-suites:
          dsl-args: "FetchSystemFiles"
      - run:
          name: cat /repo/test-clients/remote-system-files/appProperties.txt
          command: |
            cat /repo/test-clients/remote-system-files/appProperties.txt

      - run-eet-suites:
          dsl-args: "VersionInfoSpec"

  run-smart-contract-scenarios:
    parameters:
      runner:
        type: executor
        default: ci-test-executor
      workflow-name:
        type: string
        default: ""
    executor:
      name: ci-test-executor
      workflow-name:  << parameters.workflow-name >>
    steps:
      - run-eet-suites:
          dsl-args: "NewOpInConstructorSpecs"
      - run-eet-suites:
          dsl-args: "MultipleSelfDestructsAreSafe"
      - postgres-get-counts
      - run-eet-suites:
          dsl-args: "ContractCallSuite -TLS=on"
      - postgres-verify-counts:
          expected-difference: 17
      - run-eet-suites:
          dsl-args: "ContractCallLocalSuite -TLS=on"
      - run-eet-suites:
          dsl-args: "ContractUpdateSuite -TLS=on"
      - run-eet-suites:
          dsl-args: "ContractDeleteSuite -TLS=on"
      - run-eet-suites:
          dsl-args: "ChildStorageSpecs -TLS=on"
      - run-eet-suites:
          dsl-args: "BigArraySpec -TLS=on"
      - run-eet-suites:
          dsl-args: "SmartContractInlineAssemblySpec -TLS=on"
      - run-eet-suites:
          dsl-args: "OCTokenSpec -TLS=on"
      - run-eet-suites:
          dsl-args: "CharacterizationSuite -TLS=on"
      - run-eet-suites:
          dsl-args: "SmartContractFailFirstSpec -TLS=on"
      - run-eet-suites:
          dsl-args: "SmartContractSelfDestructSpec -TLS=on"
      - run-eet-suites:
          dsl-args: "ChildStorageSpecs"
      - run-eet-suites:
          dsl-args: "-A DeprecatedContractKeySpecs"
      - run-eet-suites:
          dsl-args: "ThresholdRecordCreationSpecs"
      - run-eet-suites:
          dsl-args: "ContractRecordSanityChecks"
      - run-eet-suites:
          dsl-args: "ContractGetBytecodeSuite"

  run-umbrella-freeze-restart-test:
    parameters:
      freeze-pattern-count:
        description: How many repetitions of the freeze pattern should occur
        type: integer
        default: 1
      liveness-pattern-count:
        description: How many repetitions of the liveness pattern should occur
        type: integer
        default: 3
      signed-state-pattern-count:
        description: How many repetitions of the signed state pattern should occur
        type: integer
        default: 1
      workflow-name:
        type: string
        default: ""
    executor:
      name: ci-test-executor
      workflow-name:  << parameters.workflow-name >>
    steps:
      - run:
          name: Set log level to WARN
          command: /repo/.circleci/scripts/config-log4j-4reg.sh
      - run-umbrella-test
      - run-eet-suites:
          dsl-args: "OneOfEveryTxn"
      - run-eet-suites:
          dsl-args: "MigrationValidationPreSteps"
      - start-freeze:
          failure-log-pattern-sh: /repo/.circleci/scripts/frz-fail-pattern.sh
      - run-eet-suites:
          perf-run: true
          dsl-args: "UmbrellaRedux duration=2,unit=MINUTES,maxOpsPerSec=30,props=regression-mixed_ops.properties,maxPendingOps=50"
      - validate-thaw:
          freeze-pattern-count: << parameters.freeze-pattern-count >>
      - restart-with-validations:
          valid-ss-timeout-secs: 180
          active-status-timeout-secs: 180
          liveness-pattern-count: << parameters.liveness-pattern-count >>
          signed-state-pattern-count: << parameters.signed-state-pattern-count >>
      - run-eet-suites:
          dsl-args: "MigrationValidationPostSteps"
      - run-eet-suites:
          perf-run: true
          dsl-args: "UmbrellaRedux duration=1,unit=MINUTES,maxOpsPerSec=30,props=regression-mixed_ops.properties,maxPendingOps=50"
      - run:
          name: Set log level to INFO
          command: /repo/.circleci/scripts/config-log4j-4normal.sh
      - validate-record-streams


  run-update-feature:
    parameters:
      runner:
        type: executor
        default: ci-test-executor
      workflow-name:
        type: string
        default: ""
    executor:
      name: ci-test-executor
      workflow-name:  << parameters.workflow-name >>
    steps:
      - attach_workspace:
          at: /
      - run:
          name:  Reset client log
          command: |
            cat /dev/null > /repo/test-clients/output/hapi-client.log
      - ensure-testnet-for-reruns
      - run:
          name: Set environmental variable
          command: |
            /repo/.circleci/scripts/trap-failable-for-tf-cleanup.sh \
            '/repo/.circleci/scripts/append_bash_profile.sh'
      - run-eet-suites:
          dsl-args: "CryptoTransferSuite"
      - run:
          name: wait state is saved
          command: |
            sleep 60
      - run:
          name: Show java process
          command: |
            /repo/.circleci/scripts/trap-failable-for-tf-cleanup.sh \
            '/repo/.circleci/scripts/show_java_process.sh'
      - run-eet-suites:
          dsl-args: FreezeSuite
      - run:
          name: Show logs before HGCApp restart
          command: |
            /repo/.circleci/scripts/trap-failable-for-tf-cleanup.sh \
            '/repo/.circleci/scripts/show_logs.sh'
      - run:
          name: wait HGCApp restarted
          command: |
            sleep 300
      - run:
          name: Show java process
          command: |
            /repo/.circleci/scripts/trap-failable-for-tf-cleanup.sh \
            '/repo/.circleci/scripts/show_java_process.sh'
      - run:
          name: Show logs after HGCApp restart
          command: |
            /repo/.circleci/scripts/trap-failable-for-tf-cleanup.sh \
            '/repo/.circleci/scripts/show_logs.sh'
      - run-eet-suites:
          dsl-args: "CryptoRecordSanityChecks"
      - run:
          name: Scan new setttings message
          command: |
            /repo/.circleci/scripts/trap-failable-for-tf-cleanup.sh \
            '/repo/.circleci/scripts/scan-expected-logs.sh "ERROR: fakeParameter is not a valid setting name" swirlds.log'

  run-update-jar-files:
    parameters:
      runner:
        type: executor
        default: ci-test-executor
      new-marker:
        type: string
        description: New string to indicate the chang of source code and jar
        default: 'new version jar'
      workflow-name:
        type: string
        default: ""
    executor:
      name: ci-test-executor
      workflow-name:  << parameters.workflow-name >>
    steps:
      - attach_workspace:
          at: /
      - run:
          name:  Reset client log
          command: |
            cat /dev/null > /repo/test-clients/output/hapi-client.log
      - ensure-testnet-for-reruns
      - run:
          name: Set environmental variable
          command: |
            /repo/.circleci/scripts/trap-failable-for-tf-cleanup.sh \
            '/repo/.circleci/scripts/append_bash_profile.sh'
      - run:
          name: Scan old string before changing jar files
          command: |
            /repo/.circleci/scripts/trap-failable-for-tf-cleanup.sh \
            '/repo/.circleci/scripts/scan-expected-logs.sh "init finished" hgcaa.log'
      - run:
          name: Modify main.java and build new jars
          command: |
            /repo/.circleci/scripts/trap-failable-for-tf-cleanup.sh \
            '/repo/.circleci/scripts/build_new_jars.sh "<< parameters.new-marker >>"'
      - run:
          name: Show java process before freeze
          command: |
            /repo/.circleci/scripts/trap-failable-for-tf-cleanup.sh \
            '/repo/.circleci/scripts/show_java_process.sh'
      - run-eet-suites:
          dsl-args: "UpdateServerFiles"
      - run:
          name: wait HGCApp restarted
          no_output_timeout: 4m
          command: |
            sleep 200
      - run:
          name: Show java process
          command: |
            /repo/.circleci/scripts/trap-failable-for-tf-cleanup.sh \
            '/repo/.circleci/scripts/show_java_process.sh'
      - run:
          name: Show logs after HGCApp restart
          command: |
            /repo/.circleci/scripts/trap-failable-for-tf-cleanup.sh \
            '/repo/.circleci/scripts/show_logs.sh'
      - run:
          name: Scan new string after restarted
          command: |
            /repo/.circleci/scripts/trap-failable-for-tf-cleanup.sh \
            '/repo/.circleci/scripts/scan-expected-logs.sh "<< parameters.new-marker >>" hgcaa.log'

  run-network-sim:
    parameters:
      runner:
        type: executor
        default: ci-test-executor
      workflow-name:
        type: string
        default: ""
    executor:
      name: ci-test-executor
      workflow-name:  << parameters.workflow-name >> # "simulate network outage"
    steps:
      - attach_workspace:
          at: /
      - run:
          name: call script to block tcp ports
          command: |
            /repo/.circleci/scripts/trap-failable-for-tf-cleanup.sh \
            '/repo/.circleci/scripts/firewall_create_rules.sh'
      - run-eet-suites:
          dsl-args: "CryptoTransferLoadTest status.preResolve.pause.ms=8000"
          ci-properties-map: mins=50,tps=10,threads=3
      - run:
          name: call script to remove firewall rules
          command: |
            /repo/.circleci/scripts/trap-failable-for-tf-cleanup.sh \
            '/repo/.circleci/scripts/firewall_flush_rules.sh'
      - run:
          name: Show logs after HGCApp restart
          command: |
            /repo/.circleci/scripts/show_logs.sh

  run-comprehensive-functional-test:
    parameters:
      runner:
        type: executor
        default: ci-test-executor
      workflow-name:
        type: string
        default: ""
    executor:
      name: ci-test-executor
      workflow-name:  << parameters.workflow-name >>
    steps:
      - run-eet-suites:
          dsl-args: "BucketAndLegacyThrottlingSpec"
      - run-eet-suites:
          dsl-args: "ControlAccountsExemptForUpdates"
      - run-eet-suites:
          dsl-args: "TopicCreateSpecs"
      - run-eet-suites:
          dsl-args: "SubmitMessageSpecs"
      - run-eet-suites:
          dsl-args: "TopicUpdateSpecs"
      - run-eet-suites:
          dsl-args: "TopicGetInfoSpecs"
      - run-eet-suites:
          dsl-args: "CryptoCreateSuite -TLS=on"
      - run-eet-suites:
          dsl-args: "CryptoRecordSanityChecks -TLS=on"
      - run-eet-suites:
          dsl-args: "SignedTransactionBytesRecordsSuite -TLS=on"
      - run-eet-suites:
          dsl-args: "SuperusersAreNeverThrottled"
      - run-eet-suites:
          dsl-args: "FileRecordSanityChecks"
      - run-eet-suites:
          dsl-args: "VersionInfoSpec"
      - run-eet-suites:
          dsl-args: "PermissionSemanticsSpec"
      - run-eet-suites:
          dsl-args: "SysDelSysUndelSpec"
      - run-eet-suites:
          dsl-args: "NewOpInConstructorSpecs"
      - run-eet-suites:
          dsl-args: "MultipleSelfDestructsAreSafe"
      - run-eet-suites:
          dsl-args: "FetchSystemFiles"
      - run-eet-suites:
          dsl-args: "ChildStorageSpecs"
      - run-eet-suites:
          dsl-args: "DeprecatedContractKeySpecs"
      - run-eet-suites:
          dsl-args: "ThresholdRecordCreationSpecs"
      - run-eet-suites:
          dsl-args: "ContractRecordSanityChecks"
      - run-eet-suites:
          dsl-args: "TopicDeleteSpecs"
      - run-eet-suites:
          dsl-args: "ProtectedFilesUpdateSuite"
      - run-eet-suites:
          dsl-args: "TokenCreateSpecs"
      - run-eet-suites:
          dsl-args: "TokenUpdateSpecs"
      - run-eet-suites:
          dsl-args: "TokenDeleteSpecs"
      - run-eet-suites:
          dsl-args: "TokenTransactSpecs"
      - run-eet-suites:
          dsl-args: "TokenManagementSpecs"
      - run-eet-suites:
          dsl-args: "TokenAssociationSpecs"
      - run-eet-suites:
          dsl-args: "CannotDeleteSystemEntitiesSuite"
      - run-eet-suites:
          dsl-args: "UmbrellaRedux"



  ######################################################################################################################
  # Job Name:                     jrs_regression
  # Job Version:                  1.0
  # Target Operating Systems:     Ubuntu 18.04 (bionic), Ubuntu 20.04 (focal), CentOS 7, CentOS 8
  #
  # Parameters:
  #       runtime                 a CircleCI executor reference used to control the OpenJDK and other tool versions
  #
  #
  # Description:
  #
  #   Description goes here
  #
  #
  #
  # Executor Requirements:
  #   - git (>= 2.17.1)
  #   - wget (>= 1.19.4)
  #   - zip (>= 3.0)
  #   - unzip (>= 3.0)
  #   - gzip (>= 1.6)
  #   - tar (>= 1.29)
  #   - haveged (>= 1.9.1)
  #   - maven (>= 3.6.1)
  #   - openjdk (>= 12.0.2)
  #   - postgresql-server (>= 10.9)
  #   - gcloud-sdk (>= 323.0.0)
  #
  # References:
  #   - OpenJDK:          https://jdk.java.net/
  #   - Maven:            https://maven.apache.org/
  #
  ######################################################################################################################
  jrs-regression:
    parameters:
      runtime:
        type: string
        default: ci-test-executor
      config_type:
        type: string
      config_file:
        type: string
        default: ""
      regression_path:
        type: string
        default: "regression"
      result_path:
        type: string
        default: "regression/results"
      result_bucket:
        type: string
        default: "hedera-services-jrs-test-results"
      summary_path:
        type: string
        default: "summaryHistory"
      summary_bucket:
        type: string
        default: "hedera-services-jrs-summary-history/summaryHistory"
      automated_run:
        type: boolean
        default: true
      slack_results_channel:
        type: string
        default: "hedera-regression-test"
      slack_summary_channel:
        type: string
        default: "hedera-regression-test"
      workflow-name:
        type: string
        default: ""
      hedera_services_path:
        type: string
        default: "/repo"
    executor:
      name: << parameters.runtime >>
      workflow-name:  << parameters.workflow-name >>
    steps:
      - gcp_import_credentials:
         username_variable: GCLOUD_SERVICE_ACCOUNT_NAME
         keyfile_variable: GCLOUD_SERVICE_KEY
         project_variable: GOOGLE_PROJECT_ID
      - add_ssh_keys:
          fingerprints:
            - "96:47:c4:5c:e7:45:06:c5:26:a5:85:ef:41:22:2f:d6"
            - "14:21:e9:81:1f:ae:df:ec:11:60:4a:49:e0:b9:bb:58"
            - "e7:a6:3e:34:3e:d8:fe:64:2c:7f:b6:57:45:03:44:ac"
      - when:
          condition: << parameters.automated_run >>
          steps:
            - run:
                name: setup env
                command: |
                  source /root/.bashrc; source /root/google-cloud-sdk/completion.bash.inc; source /root/google-cloud-sdk/path.bash.inc;
            - jrs_history_retrieve:
                bucket_name: << parameters.summary_bucket >>
                summary_path: << parameters.summary_path >>
      - jrs_regression_execute:
          config_file: configs/services/suites/<< parameters.config_type >>/<<parameters.workflow-name>>.json
          regression_path: << parameters.regression_path >>
          slack_results_channel: << parameters.slack_results_channel >>
          slack_summary_channel: << parameters.slack_summary_channel >>
          hedera_services_path: << parameters.hedera_services_path >>
      - when:
          condition: << parameters.automated_run >>
          steps:
            - jrs_history_store:
                bucket_name: << parameters.summary_bucket >>
                summary_path: << parameters.summary_path >>
      - jrs_results_store:
          bucket_name: << parameters.result_bucket >>
          result_path: << parameters.result_path >>
      - store_artifacts:
          path: /results.tar.gz
